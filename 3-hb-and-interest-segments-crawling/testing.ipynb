{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a77ef5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import get_engine, get_upload_url\n",
    "import pandas as pd\n",
    "from selenium.webdriver import Chrome, ChromeOptions\n",
    "from pyvirtualdisplay import Display\n",
    "import os\n",
    "import requests\n",
    "from shutil import unpack_archive, rmtree\n",
    "import json\n",
    "from datetime import datetime\n",
    "from tqdm.auto import tqdm\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3387d396",
   "metadata": {},
   "outputs": [],
   "source": [
    "crawls = pd.read_sql('crawls', con=get_engine())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c764d60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_profile(profile_name, bp='profiles'):\n",
    "    # determine output directory\n",
    "    name, ext = os.path.splitext(profile_name)\n",
    "    profile_path = os.path.join(bp, name)\n",
    "    \n",
    "    # create output directory if not exist\n",
    "    if os.path.exists(profile_path):\n",
    "        rmtree(profile_path)\n",
    "    \n",
    "    # create directory\n",
    "    os.makedirs(bp, exist_ok=True)\n",
    "    \n",
    "    # download profile zip\n",
    "    url = f'{get_upload_url()}/{profile_name}'\n",
    "    out = os.path.join(bp, profile_name)\n",
    "    r = requests.get(url)\n",
    "    with open(out, 'wb') as f:\n",
    "        f.write(r.content)\n",
    "        \n",
    "    # extract profile zip\n",
    "    unpack_archive(out, profile_path)\n",
    "    \n",
    "    # return path to profile\n",
    "    return profile_path\n",
    "\n",
    "def load_driver(profile_name=None, headless=False):\n",
    "    # download profile from server\n",
    "    options = ChromeOptions()\n",
    "    options.add_argument('--window-size=1920,1080')\n",
    "    options.add_argument('--no-sandbox')\n",
    "    \n",
    "    if headless:\n",
    "        options.add_argument('--headless')\n",
    "    \n",
    "    if profile_name is not None:\n",
    "        user_data_dir = download_profile(profile_name)\n",
    "        options.add_argument(f'--user-data-dir={user_data_dir}')\n",
    "        \n",
    "    driver = Chrome(options=options)\n",
    "    driver.implicitly_wait(30)\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02805201",
   "metadata": {},
   "source": [
    "## Interest Segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271e2d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_interest_categories(profile_name):\n",
    "    driver = load_driver(profile_name, headless=True)\n",
    "    driver.get('https://registry.bluekai.com/get_categories')\n",
    "    # find tag containing response\n",
    "    body = driver.find_elements_by_tag_name('pre')[0]\n",
    "    interest_categories = json.loads(body.text)\n",
    "\n",
    "    driver.close()\n",
    "    return interest_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffd6a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "fails = []\n",
    "for crawl in tqdm(crawls.itertuples(), total=crawls.shape[0]):\n",
    "    try:\n",
    "        ic = get_interest_categories(crawl.Filename)\n",
    "        print(crawl.Category, len(ic))\n",
    "        # write to database\n",
    "        df = pd.DataFrame(ic, columns=['Segment'])\n",
    "        df['Profile'] = crawl.Filename\n",
    "        df['Time'] = datetime.now()\n",
    "        df.to_sql('interest-categories', con=get_engine(), index=False, if_exists='append')\n",
    "    except:\n",
    "        fails.append(crawl.Filename)\n",
    "        print(\"Error while processing\", crawl.Filename, ic)\n",
    "        \n",
    "for f in tqdm(fails):\n",
    "    ic = get_interest_categories(f)\n",
    "    # write to database\n",
    "    df = pd.DataFrame(ic, columns=['Segment'])\n",
    "    df['Profile'] = crawl.Filename\n",
    "    df['Time'] = datetime.now()\n",
    "    df.to_sql('interest-categories', con=get_engine(), index=False, if_exists='append')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888362dc",
   "metadata": {},
   "source": [
    "## Bid Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df4a14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHBInfo(profile_name, site=None):\n",
    "    driver = load_driver(None, headless=False)\n",
    "    driver.get('https://%s' % site)\n",
    "    \n",
    "    row = {\n",
    "        'Profile': profile_name,\n",
    "        'HB_URL': site\n",
    "    }\n",
    "    # find pbjs var\n",
    "    pbjsGlobal = driver.execute_script('return _pbjsGlobals')[0]\n",
    "    # find methods for pbjs\n",
    "    methods = driver.execute_script('return Object.keys(%s).filter(x => typeof(%s[x]) == \"function\")' % (pbjsGlobal, pbjsGlobal))\n",
    "    # store each methods response\n",
    "    for method in methods:\n",
    "        try:\n",
    "            row[method] = json.dumps(driver.execute_script('return %s.%s()' % (pbjsGlobal, method)))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    driver.close()\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a479e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "hb_sites = pd.read_sql('SELECT DISTINCT sw.URL, Category FROM similarweb sw JOIN `header-bidding-sites` hb ON sw.URL = hb.URL', con=get_engine())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcf0486",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for crawl in tqdm(crawls.itertuples(), total=crawls.shape[0]):\n",
    "    if crawl.Category =='Adult':\n",
    "        continue\n",
    "    # for each crawl, check its category\n",
    "    crawl_category = crawl.Category\n",
    "    \n",
    "    # organize sites by category\n",
    "    sites = hb_sites[hb_sites['Category'] == crawl_category]['URL'].to_list() + hb_sites[hb_sites['Category'] != crawl_category]['URL'].to_list()\n",
    "    print(crawl_category, sites)\n",
    "    hb_responses = []\n",
    "    \n",
    "    for site in sites:\n",
    "        hb_responses.append(getHBInfo(crawl.Filename, site))\n",
    "    \n",
    "    pd.DataFrame(hb_responses, 'header-bidding-responses', con=get_engine(), index=False, if_exists='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d11e7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "crawl_category = 'Vehicles'\n",
    "hb_sites[hb_sites['Category'] == crawl_category]['URL'].to_list() + hb_sites[hb_sites['Category'] != crawl_category]['URL'].to_list()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c61d8db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}